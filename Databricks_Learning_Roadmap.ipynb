{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059f3b05",
   "metadata": {},
   "source": [
    "# Databricks Learning Roadmap\n",
    "\n",
    "## Overview\n",
    "Databricks is a unified data analytics platform built on Apache Spark. This notebook provides a comprehensive roadmap for learning Databricks from scratch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0dc27",
   "metadata": {},
   "source": [
    "# Phase 1: Foundational Concepts\n",
    "\n",
    "## 1.1 What is Databricks?\n",
    "- **Definition**: A managed platform that combines data engineering, data science, and business analytics\n",
    "- **Core Technology**: Built on Apache Spark\n",
    "- **Key Features**:\n",
    "  - Collaborative workspace\n",
    "  - Multi-language support (Python, SQL, Scala, R)\n",
    "  - Delta Lake for data management\n",
    "  - Machine Learning capabilities\n",
    "  - Built-in notebooks and dashboards\n",
    "\n",
    "## 1.2 Core Components\n",
    "1. **Workspace**: Collaborative environment for notebooks and dashboards\n",
    "2. **Clusters**: Compute resources (Apache Spark clusters)\n",
    "3. **Notebooks**: Interactive documents supporting multiple languages\n",
    "4. **Jobs**: Scheduled or triggered workflows\n",
    "5. **Delta Lake**: ACID-compliant table storage format\n",
    "6. **SQL Analytics**: Query engine for data warehousing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa84abe",
   "metadata": {},
   "source": [
    "# Phase 2: Getting Started\n",
    "\n",
    "## 2.1 Account Setup\n",
    "- [ ] Create Databricks account (Community Edition or Trial)\n",
    "- [ ] Log in to Databricks workspace\n",
    "- [ ] Familiarize with the UI\n",
    "\n",
    "## 2.2 Workspace Navigation\n",
    "- **Sidebar**: Access notebooks, clusters, jobs, and more\n",
    "- **Home**: Your personal workspace\n",
    "- **Shared**: Shared resources with team\n",
    "- **Data**: Browse tables and databases\n",
    "- **Compute**: Manage clusters\n",
    "\n",
    "## 2.3 Create Your First Cluster\n",
    "- Steps:\n",
    "  1. Go to Compute â†’ Create Cluster\n",
    "  2. Configure cluster settings (workers, node types)\n",
    "  3. Install libraries if needed\n",
    "  4. Start the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701e584",
   "metadata": {},
   "source": [
    "# Phase 3: Databricks Notebooks\n",
    "\n",
    "## 3.1 Notebook Basics\n",
    "- **What**: Interactive documents combining code, visualizations, and markdown\n",
    "- **Supported Languages**: Python, SQL, Scala, R\n",
    "- **Cell Types**:\n",
    "  - Code cells (executable)\n",
    "  - Markdown cells (documentation)\n",
    "  - Commands (%python, %sql, %scala, %md)\n",
    "\n",
    "## 3.2 Working with Notebooks\n",
    "- Create new notebook: Home â†’ New â†’ Notebook\n",
    "- Attach to cluster before running code\n",
    "- Use Cmd/Ctrl + Enter to execute cells\n",
    "- Mix languages using magic commands (%python, %sql)\n",
    "- Share notebooks with team members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa660b",
   "metadata": {},
   "source": [
    "# Phase 4: Apache Spark Fundamentals\n",
    "\n",
    "## 4.1 Spark Basics\n",
    "- **RDD**: Resilient Distributed Datasets (low-level)\n",
    "- **DataFrame**: Distributed collection of data organized into columns (high-level)\n",
    "- **Schema**: Structure of the DataFrame\n",
    "- **Partitions**: Data splits across cluster nodes\n",
    "\n",
    "## 4.2 Key Concepts\n",
    "- **Lazy Evaluation**: Spark doesn't execute until an action is called\n",
    "- **Transformations**: Operations that return new DataFrames (map, filter, select, join)\n",
    "- **Actions**: Operations that trigger computation (collect, show, count, write)\n",
    "- **Wide vs Narrow Transformations**: Affect shuffle operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Basic Spark DataFrame Operations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg\n",
    "\n",
    "# Note: In Databricks, SparkSession is pre-initialized as 'spark'\n",
    "# spark = SparkSession.builder.appName(\"Learning\").getOrCreate()\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [\n",
    "    (\"Alice\", 25, 50000),\n",
    "    (\"Bob\", 30, 60000),\n",
    "    (\"Charlie\", 35, 75000),\n",
    "    (\"Diana\", 28, 55000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"salary\"]\n",
    "\n",
    "# Create DataFrame from data\n",
    "# df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Display DataFrame\n",
    "# df.show()\n",
    "\n",
    "print(\"Example DataFrame structure:\")\n",
    "print(f\"Columns: {columns}\")\n",
    "print(f\"Number of rows: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afcb0e5",
   "metadata": {},
   "source": [
    "# Phase 5: Delta Lake\n",
    "\n",
    "## 5.1 What is Delta Lake?\n",
    "- **Purpose**: ACID-compliant storage layer on top of data lake\n",
    "- **Advantages**:\n",
    "  - ACID transactions\n",
    "  - Schema enforcement\n",
    "  - Time travel (version history)\n",
    "  - Unified batch and streaming\n",
    "  - Data quality checks\n",
    "\n",
    "## 5.2 Delta Lake Operations\n",
    "- **CREATE**: Create Delta table\n",
    "- **WRITE**: Write data to Delta table (OVERWRITE, APPEND, MERGE)\n",
    "- **READ**: Query Delta table\n",
    "- **UPDATE**: Modify existing records\n",
    "- **DELETE**: Remove records\n",
    "- **VACUUM**: Clean up old versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8775ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Basic Delta Lake Operations\n",
    "\n",
    "# 1. Write DataFrame to Delta table\n",
    "# df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").format(\"delta\").save(\"/delta/my_table\")\n",
    "\n",
    "# 2. Read from Delta table\n",
    "# delta_df = spark.read.format(\"delta\").load(\"/delta/my_table\")\n",
    "\n",
    "# 3. Create managed Delta table\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"my_managed_table\")\n",
    "\n",
    "# 4. SQL operations on Delta table\n",
    "# spark.sql(\"SELECT * FROM my_managed_table WHERE age > 30\")\n",
    "\n",
    "# 5. Check table history\n",
    "# spark.sql(\"DESCRIBE HISTORY my_managed_table\")\n",
    "\n",
    "print(\"Delta Lake - Common Operations:\")\n",
    "print(\"1. Write (Overwrite/Append/Merge)\")\n",
    "print(\"2. Read (Load from path or table)\")\n",
    "print(\"3. Update/Delete (Modify records)\")\n",
    "print(\"4. Time Travel (Access previous versions)\")\n",
    "print(\"5. Vacuum (Clean old versions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6cdc86",
   "metadata": {},
   "source": [
    "# Phase 6: SQL in Databricks\n",
    "\n",
    "## 6.1 Databricks SQL\n",
    "- Query data using standard SQL\n",
    "- SQL Analytics warehouse for BI queries\n",
    "- Optimized query performance\n",
    "- Integration with business intelligence tools\n",
    "\n",
    "## 6.2 Common SQL Operations\n",
    "- CREATE TABLE/DATABASE\n",
    "- SELECT, WHERE, JOIN, GROUP BY\n",
    "- Aggregations (SUM, AVG, COUNT)\n",
    "- Window functions\n",
    "- CTEs (Common Table Expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: SQL Magic Commands in Databricks Notebooks\n",
    "# (Run these in actual Databricks notebook)\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE employees (\n",
    "#     emp_id INT,\n",
    "#     name STRING,\n",
    "#     department STRING,\n",
    "#     salary DECIMAL(10,2),\n",
    "#     hire_date DATE\n",
    "# ) USING DELTA;\n",
    "\n",
    "# %sql\n",
    "# SELECT department, COUNT(*) as emp_count, AVG(salary) as avg_salary\n",
    "# FROM employees\n",
    "# GROUP BY department\n",
    "# ORDER BY avg_salary DESC;\n",
    "\n",
    "print(\"SQL operations in Databricks:\")\n",
    "print(\"- DDL: CREATE, ALTER, DROP\")\n",
    "print(\"- DML: INSERT, UPDATE, DELETE\")\n",
    "print(\"- DQL: SELECT with various clauses\")\n",
    "print(\"- Advanced: Window functions, CTEs, Subqueries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e428df",
   "metadata": {},
   "source": [
    "# Phase 7: Data Processing with PySpark\n",
    "\n",
    "## 7.1 Common Operations\n",
    "- **Select**: Choose specific columns\n",
    "- **Filter**: Apply conditions\n",
    "- **Join**: Combine multiple DataFrames\n",
    "- **GroupBy**: Aggregate data\n",
    "- **Window Functions**: Row-based calculations\n",
    "- **UDF**: User Defined Functions\n",
    "\n",
    "## 7.2 Data Import/Export\n",
    "- Read from CSV, JSON, Parquet\n",
    "- Read from external databases (JDBC)\n",
    "- Write to various formats\n",
    "- Stream from Kafka, Azure Event Hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525bf9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: PySpark Data Processing\n",
    "\n",
    "# Reading data\n",
    "# df = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True)\n",
    "# df = spark.read.format(\"parquet\").load(\"/path/to/parquet\")\n",
    "\n",
    "# Transformations\n",
    "# df_filtered = df.filter(col(\"age\") > 25)\n",
    "# df_selected = df.select(\"name\", \"salary\")\n",
    "# df_grouped = df.groupBy(\"department\").agg({\"salary\": \"avg\"})\n",
    "\n",
    "# Joins\n",
    "# df_joined = df1.join(df2, on=\"emp_id\", how=\"inner\")\n",
    "\n",
    "# Writing data\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/processed_data\")\n",
    "\n",
    "print(\"PySpark operations examples:\")\n",
    "print(\"âœ“ Read CSV/JSON/Parquet\")\n",
    "print(\"âœ“ Filter, Select, GroupBy\")\n",
    "print(\"âœ“ Join multiple DataFrames\")\n",
    "print(\"âœ“ Window functions and aggregations\")\n",
    "print(\"âœ“ Write to Delta/Parquet format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d1a4f",
   "metadata": {},
   "source": [
    "# Phase 8: Machine Learning with Databricks\n",
    "\n",
    "## 8.1 MLlib Overview\n",
    "- **Purpose**: Spark's machine learning library\n",
    "- **Algorithms**: Classification, Regression, Clustering\n",
    "- **Features**: Feature engineering, pipelines\n",
    "\n",
    "## 8.2 MLflow\n",
    "- **Tracking**: Log parameters, metrics, models\n",
    "- **Projects**: Package code as reproducible projects\n",
    "- **Models**: Registry and versioning\n",
    "- **Serving**: Deploy models as REST endpoints\n",
    "\n",
    "## 8.3 Key ML Concepts\n",
    "- Feature engineering and transformation\n",
    "- Train/test split\n",
    "- Model evaluation metrics\n",
    "- Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3623bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Basic MLlib Usage\n",
    "\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Prepare features\n",
    "# assembler = VectorAssembler(inputCols=[\"age\", \"experience\"], outputCol=\"features\")\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# # Train model\n",
    "# lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"salary\")\n",
    "\n",
    "# # Create pipeline\n",
    "# pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "# model = pipeline.fit(training_data)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = model.transform(test_data)\n",
    "\n",
    "# # Evaluate\n",
    "# evaluator = RegressionEvaluator(labelCol=\"salary\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "# rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"ML Workflow:\")\n",
    "print(\"1. Data preparation and feature engineering\")\n",
    "print(\"2. Train/test split\")\n",
    "print(\"3. Model training\")\n",
    "print(\"4. Evaluation and tuning\")\n",
    "print(\"5. Model deployment with MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbff4f",
   "metadata": {},
   "source": [
    "# Phase 9: Workflows and Jobs\n",
    "\n",
    "## 9.1 Databricks Jobs\n",
    "- **Purpose**: Schedule and run notebooks or code as workflows\n",
    "- **Triggers**: Manual, scheduled (cron), event-based\n",
    "- **Monitoring**: View run history, logs, alerts\n",
    "- **Clusters**: Use existing or create new cluster for job\n",
    "\n",
    "## 9.2 Creating a Job\n",
    "1. Go to Workflows â†’ Jobs\n",
    "2. Create new job\n",
    "3. Select notebook or JAR\n",
    "4. Configure cluster and parameters\n",
    "5. Set schedule/trigger\n",
    "6. Enable alerts\n",
    "\n",
    "## 9.3 Best Practices\n",
    "- Use parameterized notebooks\n",
    "- Set appropriate cluster sizes\n",
    "- Monitor job performance\n",
    "- Implement error handling and notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14ed2f",
   "metadata": {},
   "source": [
    "# Phase 10: Advanced Topics\n",
    "\n",
    "## 10.1 Streaming\n",
    "- **Structured Streaming**: Process real-time data streams\n",
    "- **Sources**: Kafka, Azure Event Hubs, Socket\n",
    "- **Sinks**: Console, memory, file, Kafka, Delta\n",
    "\n",
    "## 10.2 Performance Optimization\n",
    "- **Partitioning**: Data organization for efficient querying\n",
    "- **Caching**: In-memory caching of DataFrames\n",
    "- **Broadcasting**: Broadcast small DataFrames to all nodes\n",
    "- **Bucketing**: Pre-sort and group data\n",
    "- **Adaptive Query Execution**: Automatically optimize queries\n",
    "\n",
    "## 10.3 Security\n",
    "- **Access Control**: Workspace, cluster, table level\n",
    "- **Authentication**: SSO, OAuth, PAT tokens\n",
    "- **Encryption**: At rest and in transit\n",
    "- **Audit Logs**: Track user actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab204119",
   "metadata": {},
   "source": [
    "# Phase 11: Learning Resources\n",
    "\n",
    "## Official Documentation\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [PySpark API Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Delta Lake Documentation](https://docs.delta.io/)\n",
    "\n",
    "## Recommended Learning Path\n",
    "1. Week 1: Databricks basics, workspace setup, notebooks\n",
    "2. Week 2: Apache Spark fundamentals, DataFrames\n",
    "3. Week 3: Delta Lake and SQL\n",
    "4. Week 4: Data processing with PySpark\n",
    "5. Week 5: Jobs, workflows, and scheduling\n",
    "6. Week 6: Machine learning with MLlib and MLflow\n",
    "7. Week 7-8: Advanced topics and optimization\n",
    "\n",
    "## Hands-on Exercises\n",
    "- Create sample datasets and practice transformations\n",
    "- Build end-to-end data pipelines\n",
    "- Train ML models and track with MLflow\n",
    "- Optimize slow queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59372b79",
   "metadata": {},
   "source": [
    "# Phase 12: Project Ideas\n",
    "\n",
    "## Beginner Projects\n",
    "1. **Data Cleaning Pipeline**: Load messy data, clean and transform\n",
    "2. **Sales Analytics**: Analyze sales data with SQL and visualizations\n",
    "3. **Data Quality Checks**: Build validation rules for data\n",
    "\n",
    "## Intermediate Projects\n",
    "1. **ETL Pipeline**: Extract, transform, load data from multiple sources\n",
    "2. **Time Series Analysis**: Analyze temporal data patterns\n",
    "3. **Customer Segmentation**: Use clustering to segment customers\n",
    "\n",
    "## Advanced Projects\n",
    "1. **Real-time Streaming Pipeline**: Process streaming data\n",
    "2. **ML Model Pipeline**: End-to-end ML workflow with MLflow\n",
    "3. **Data Warehouse**: Build multi-dimensional data models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42624e66",
   "metadata": {},
   "source": [
    "# Learning Checklist\n",
    "\n",
    "## Phase 1-2: Foundations\n",
    "- [ ] Understand Databricks platform and components\n",
    "- [ ] Create Databricks account\n",
    "- [ ] Create and configure cluster\n",
    "- [ ] Create first notebook\n",
    "\n",
    "## Phase 3-4: Core Skills\n",
    "- [ ] Master notebook interface\n",
    "- [ ] Understand Spark fundamentals\n",
    "- [ ] Learn RDD vs DataFrame vs Dataset\n",
    "- [ ] Practice transformations and actions\n",
    "\n",
    "## Phase 5-6: Data Management\n",
    "- [ ] Understand Delta Lake concepts\n",
    "- [ ] Create and manage Delta tables\n",
    "- [ ] Write SQL queries in Databricks\n",
    "- [ ] Practice ACID transactions\n",
    "\n",
    "## Phase 7-8: Data Processing & ML\n",
    "- [ ] Master PySpark transformations\n",
    "- [ ] Build data processing pipelines\n",
    "- [ ] Train ML models\n",
    "- [ ] Track experiments with MLflow\n",
    "\n",
    "## Phase 9-10: Production\n",
    "- [ ] Create and schedule jobs\n",
    "- [ ] Monitor performance\n",
    "- [ ] Optimize queries\n",
    "- [ ] Implement streaming workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9288a7",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "## Remember:\n",
    "1. **Databricks** = Apache Spark + managed services + collaboration tools\n",
    "2. **Delta Lake** = ACID-compliant, versioned, reliable data storage\n",
    "3. **Notebooks** = Interactive, collaborative development environment\n",
    "4. **Spark** = Distributed data processing framework\n",
    "5. **MLflow** = Model tracking, versioning, and deployment\n",
    "6. **SQL** = Query language for analytics and business intelligence\n",
    "7. **Jobs** = Automated workflows and scheduling\n",
    "8. **Streaming** = Real-time data processing\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps:\n",
    "âœ“ Start with Phase 1 and progress sequentially\n",
    "âœ“ Hands-on practice is more important than theory\n",
    "âœ“ Build small projects after each phase\n",
    "âœ“ Join Databricks community forums\n",
    "âœ“ Keep learning and experimenting!\n",
    "\n",
    "**Happy Learning! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
